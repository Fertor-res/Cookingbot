{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433f3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q llama-index-core\n",
    "! pip install -q llama-index-llms-groq\n",
    "! pip install -q llama-index-readers-file\n",
    "! pip install -q llama-index-embeddings-huggingface\n",
    "! pip install -q llama-index-embeddings-instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68179ad",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb38d7",
   "metadata": {},
   "source": [
    "### Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fd1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=800, chunk_overlap=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9eb8d",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b2256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_folder = r\"./embedding_model/\" # \"C:/Users/luisf/OneDrive/FERNANDO/Training/Data Science BootCamp/10_Generative_AI/Exercises/embedding_model/\"\n",
    "\n",
    "embeddings = HuggingFaceEmbedding(\n",
    "    model_name = embedding_model, cache_folder= embedding_folder\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e199de0",
   "metadata": {},
   "source": [
    "### Creating a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9d975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./\", required_exts=[\".pdf\"]).load_data() # loads only -txt documents from the active directory\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[text_splitter], embed_model = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0ce384",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index.storage_context.persist(persist_dir=\"./vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6990f",
   "metadata": {},
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba0262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rag_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rag_app.py\n",
    "\n",
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.chat_engine import ContextChatEngine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "import streamlit as st\n",
    "\n",
    "# configure the page \n",
    "st.set_page_config(\n",
    "    page_title=\"Find your recipes!\",\n",
    "    page_icon=\"üç≥\",\n",
    "    layout=\"centered\",\n",
    ")\n",
    "# Apply colors via CCS\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    ":root {\n",
    "    --primary-color: #f49a0aff;\n",
    "    --background-color: #ffffffaa;\n",
    "    --text-color: #056ce9ff;\n",
    "}\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# Changing the chat background for an image:\n",
    "from pathlib import Path\n",
    "import base64\n",
    "\n",
    "# Function to change the entire app background\n",
    "def set_app_background(image_file):\n",
    "    with open(image_file, \"rb\") as f:\n",
    "        encoded = base64.b64encode(f.read()).decode()\n",
    "\n",
    "    css = f\"\"\"\n",
    "    <style>\n",
    "    .stApp {{\n",
    "        background-image: url(\"data:image/jpg;base64,{encoded}\");\n",
    "        background-size: cover;\n",
    "        background-position: center;\n",
    "        background-repeat: no-repeat;\n",
    "        background-attachment: fixed;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    st.markdown(css, unsafe_allow_html=True)\n",
    "\n",
    "def set_chat_background(image_file):\n",
    "    import base64\n",
    "    with open(image_file, \"rb\") as f:\n",
    "        encoded = base64.b64encode(f.read()).decode()\n",
    "\n",
    "    css = f\"\"\"\n",
    "    <style>\n",
    "    .stChatMessage {{\n",
    "        /* Layer 1: gradient overlay, Layer 2: your image */\n",
    "        background-image:\n",
    "            linear-gradient(to bottom, rgba(255,255,255,0.9), rgba(255,255,255,0.5)),\n",
    "            url(\"data:image/jpg;base64,{encoded}\");\n",
    "        background-size: cover;\n",
    "        background-position: center;\n",
    "        background-repeat: no-repeat;\n",
    "        border-radius: 10px;\n",
    "        padding: 10px;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    st.markdown(css, unsafe_allow_html=True)\n",
    "\n",
    "# Full app background\n",
    "full_bg_path = Path(__file__).parent / \"kitchen_wall.jpg\"\n",
    "set_app_background(str(full_bg_path))\n",
    "\n",
    "# Set the chat background\n",
    "image_path = Path(__file__).parent / \"kitchen_background.jpg\"\n",
    "set_chat_background(str(image_path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### INITIALIZING AND CACHING CHATBOT COMPONENTS ###\n",
    "\n",
    "# Function for initializing the LLM\n",
    "@st.cache_resource #the result will be cached so it only has to rerun when temp changes\n",
    "def init_llm(temp=0.01):\n",
    "    # LLM\n",
    "    return Groq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    max_new_tokens=768,\n",
    "    temperature=temp,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.03,\n",
    "    token=st.secrets[\"GROQ_API_KEY\"]\n",
    "    )\n",
    "\n",
    "# Function for initializing the retriever\n",
    "@st.cache_resource #the result will be cached so it only has to rerun when num_chunks changes\n",
    "def init_rag(num_chunks=2):\n",
    "    # RAG\n",
    "    embeddings = HuggingFaceEmbedding(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        #cache_folder=\"./embedding_model/\",\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./vector_index\")\n",
    "    vector_index = load_index_from_storage(storage_context, embed_model=embeddings)\n",
    "    return vector_index.as_retriever(similarity_top_k=num_chunks)\n",
    "\n",
    "\n",
    "# Function for initializing the chatbot memory\n",
    "@st.cache_resource #the result will be cached so it only has to run once\n",
    "def init_memory():\n",
    "    return ChatMemoryBuffer.from_defaults()\n",
    "\n",
    "\n",
    "# Function for initializing the bot with the specific settings\n",
    "@st.cache_resource\n",
    "def init_bot(prefix_messages, temp=0.01, num_chunks=2):\n",
    "    # Initialize components\n",
    "    llm = init_llm(temp)\n",
    "    retriever = init_rag(num_chunks)\n",
    "    memory = init_memory()\n",
    "\n",
    "    # Build ChatMessage list safely\n",
    "    safe_prefix_messages = []\n",
    "    for system_prompt_selection in prefix_messages:\n",
    "        if system_prompt_selection in prompt_options:\n",
    "            # Use the predefined prompt text\n",
    "            content = prompt_options[system_prompt_selection]\n",
    "        else:\n",
    "            # Use the string as-is (dynamic prompt)\n",
    "            content = system_prompt_selection\n",
    "\n",
    "        safe_prefix_messages.append(\n",
    "            ChatMessage(\n",
    "                role=MessageRole.SYSTEM,\n",
    "                content=content\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Return initialized bot\n",
    "    return ContextChatEngine(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        prefix_messages=safe_prefix_messages\n",
    "    )\n",
    "\n",
    "\n",
    "    ##### STREAMLIT #####\n",
    "\n",
    "st.title(\"Your Virtual Chef!\")\n",
    "\n",
    "\n",
    "### PROMPT CUSTOMIZATION ###\n",
    "\n",
    "\n",
    "# Create two side by side columns\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "# Cooking confidence (Beginner/Expert) -- Using radio\n",
    "\n",
    "with col1:\n",
    "    st.radio('How confident are you with cooking? ', ['Beginner', 'Expert'], key= \"cooking_mode\")\n",
    "    \n",
    "\n",
    "# Recipe type (Starter / Main Dish / Dessert) ‚Äî using segmented control\n",
    "\n",
    "with col2:\n",
    "    st.segmented_control(\n",
    "        \"Recipe type:\",\n",
    "        [\"Starter\", \"Main Dish\", \"Dessert\"],\n",
    "        key=\"recipe_type\",\n",
    "        default=\"Main Dish\"  # üëà ensures a value immediately\n",
    "    )\n",
    "\n",
    "selected_mode = st.session_state.get(\"cooking_mode\", \"Beginner\")\n",
    "selected_type = st.session_state.get(\"recipe_type\", \"Main Dish\")\n",
    "\n",
    "st.session_state['system_prompts'] = [\n",
    "    \"basic_context\",\n",
    "    selected_mode,\n",
    "    f\"You are preparing a {selected_type.lower()} recipe. Adjust your suggestions accordingly.\"\n",
    "]\n",
    "\n",
    "# Setting up session state to store current system prompt setting\n",
    "if 'system_prompts' not in st.session_state:\n",
    "    st.session_state['system_prompts'] = ['basic_context', selected_mode, f\"You are preparing a {selected_type.lower()} recipe. Adjust your suggestions accordingly.\"\n",
    "] #making it a list allow it to have multiple at once\n",
    "\n",
    "\n",
    "# Setting up system prompt options:\n",
    "prompt_options = {\n",
    "    'basic_context': (\n",
    "        'You are a chatbot with two modes: Beginner and Expert. '\n",
    "        f\"You are preparing a {selected_type.lower()} recipe. Adjust your suggestions accordingly.\"\n",
    "        'You are a helpful chatbot having a conversation with a human. '\n",
    "        'Give priority to the recipes feed to you in the .pdf files provided to you'\n",
    "        \"Everytime you are queried to do something outside the topic cooking, answer with 'Sorry, I can only cook'. Do not answer outside the world of cooking.\"\n",
    "        \"At the bottom of the recipe, please provide the source from where you took the information\"\n",
    "        ),\n",
    "    'Beginner': (\n",
    "        'YOU ARE NOW IN BEGINNER MODE, change your behavior if needed. '\n",
    "        'You are a helpful chatbot having a conversation with a human. '\n",
    "        'Look for easy to cook recipes. Ask always the number of people the user will cook for. Adjust your answer to that number of people. Present the ingredients first in bullet points, after that, the cooking instructions as detailed as possible and include preparation times'\n",
    "        ),\n",
    "    'Expert': (\n",
    "        'YOU ARE NOW IN EXPERT MODE, change your behavior if needed. '\n",
    "        'Search for complex recipes. Ask always the number of people the user will cook for. Adjust your answer to that number of people. Present the ingredients first in bullet points, after that, the cooking instructions with a low level of detail unless the user specify otherwise and include preparation times '\n",
    "        )\n",
    "}\n",
    "\n",
    "\n",
    "### CHAT ###\n",
    "\n",
    "# Initializing chatbot\n",
    "# If the parameters change, this reruns, otherwise it uses what is in the cache already\n",
    "rag_bot = init_bot(\n",
    "    prefix_messages=st.session_state['system_prompts'],\n",
    "    temp=0.5,\n",
    "    num_chunks=2\n",
    ")\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in rag_bot.chat_history:\n",
    "    with st.chat_message(message.role):\n",
    "        st.markdown(message.blocks[0].text)\n",
    "\n",
    "\n",
    "# React to user input\n",
    "if prompt := st.chat_input('Reset the chat by typing \"Goodbye\"'):\n",
    "\n",
    "    # If user types \"goodbye\", reset the memory and run the app from the top again\n",
    "    if prompt.lower() == 'goodbye':\n",
    "        rag_bot.reset() # reset the bot memory\n",
    "        st.rerun() # reruns the app so that the bot is reinitialized and the chat is cleared\n",
    "    \n",
    "    # Display user message in chat message container\n",
    "    st.chat_message(\"human\").markdown(prompt)\n",
    "\n",
    "    # Begin spinner before answering question so it's there for the duration\n",
    "    with st.spinner(\"Be patient, a good meal requires always time!...\"):\n",
    "        # send question to bot to get answer\n",
    "        answer = rag_bot.chat(prompt)\n",
    "\n",
    "        # extract answer from bot's response\n",
    "        response = answer.response\n",
    "\n",
    "        # Display chatbot response in chat message container\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(response)\n",
    "# Use streamlit run rag_app.py in Terminal to run this Python code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
