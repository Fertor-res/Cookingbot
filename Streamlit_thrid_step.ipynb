{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this from the Colab Notebook is not running here\n",
    "    # %%bash\n",
    "    # uv pip install -q llama-index-core\n",
    "    # uv pip install -q llama-index-llms-groq\n",
    "    # uv pip install -q llama-index-readers-file\n",
    "    # uv pip install -q llama-index-embeddings-huggingface\n",
    "    # uv pip install -q llama-index-embeddings-instructor\n",
    "\n",
    "\n",
    "\n",
    "! pip install -q llama-index-core\n",
    "! pip install -q llama-index-llms-groq\n",
    "! pip install -q llama-index-readers-file\n",
    "! pip install -q llama-index-embeddings-huggingface\n",
    "! pip install -q llama-index-embeddings-instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41de5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os to have interaction with the operating system and be able to assign vairables in the os.\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d655cf",
   "metadata": {},
   "source": [
    "# Find the data (just for the class example, where a txt file was copied from the Internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6be07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to: C:\\Users\\luisf\\OneDrive\\FERNANDO\\Training\\Data Science BootCamp\\10_Generative_AI\\Exercises\\alice_in_wonderland.txt\n"
     ]
    }
   ],
   "source": [
    "# Code from Colab which needs to be adapted to be used in a local machine.\n",
    "# %mkdir -p /content/data\n",
    "# !wget -O /content/data/alice_in_wonderland.txt https://www.gutenberg.org/cache/epub/11/pg11.txt\n",
    "\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "# Define the download URL\n",
    "url = \"https://www.gutenberg.org/cache/epub/11/pg11.txt\"\n",
    "\n",
    "# Define the destination path in the current directory\n",
    "destination = Path().resolve() / \"alice_in_wonderland.txt\"\n",
    "\n",
    "# Download the file\n",
    "urllib.request.urlretrieve(url, destination)\n",
    "\n",
    "print(f\"File downloaded to: {destination}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01a149e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'destination' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m destination\n",
      "\u001b[1;31mNameError\u001b[0m: name 'destination' is not defined"
     ]
    }
   ],
   "source": [
    "destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68179ad",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb38d7",
   "metadata": {},
   "source": [
    "### Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2fd1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=800, chunk_overlap=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9eb8d",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8b2256f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 10:23:56,284 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 10:23:59,273 - INFO - 2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_folder = r\"./embedding_model/\" # \"C:/Users/luisf/OneDrive/FERNANDO/Training/Data Science BootCamp/10_Generative_AI/Exercises/embedding_model/\"\n",
    "\n",
    "embeddings = HuggingFaceEmbedding(\n",
    "    model_name = embedding_model, cache_folder= embedding_folder\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e199de0",
   "metadata": {},
   "source": [
    "### Creating a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b9d975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./\", required_exts=[\".pdf\"]).load_data() # loads only -txt documents from the active directory\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[text_splitter], embed_model = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4e550",
   "metadata": {},
   "source": [
    "+ Once the database is made, you can save it to use over and over again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a0ce384",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index.storage_context.persist(persist_dir=\"./vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6990f",
   "metadata": {},
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc8fba3",
   "metadata": {},
   "source": [
    "## Build the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3495be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rag_app.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile rag_app.py\n",
    "\n",
    "\n",
    "# from llama_index.llms.groq import Groq\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.core import StorageContext, load_index_from_storage\n",
    "# from llama_index.core.chat_engine import ContextChatEngine\n",
    "# from llama_index.core.memory import ChatMemoryBuffer\n",
    "# from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "# import streamlit as st\n",
    "\n",
    "# # Secrets\n",
    "\n",
    "\n",
    "\n",
    "# # prompt\n",
    "# prefix_messages = [\n",
    "#      ChatMessage(\n",
    "#         role=MessageRole.SYSTEM,\n",
    "#         content=\"You are a nice chatbot having a conversation with a human.\",\n",
    "#     ),\n",
    "#     ChatMessage(\n",
    "#         role=MessageRole.SYSTEM, content=\"Keep your answers short and succinct.\"\n",
    "#     ),\n",
    "#     ChatMessage(\n",
    "#         role=MessageRole.SYSTEM,\n",
    "#         content=\"You are a cooking chef specialized in south american and geman cousine.\",\n",
    "#     ),\n",
    "#    ChatMessage(\n",
    "#         role=MessageRole.SYSTEM,\n",
    "#         content=\"You will ask the user if it wants to have the answer in a different language than english.\",\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# # bot with memory\n",
    "# @st.cache_resource # this is called a decorator. In this case allows to use the cache to store memory, chat memory in the case.\n",
    "# def init_bot():\n",
    "#     # llm\n",
    "#     model = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "#     llm = Groq(\n",
    "#         model=model,\n",
    "#         token=st.secrets['GROQ_API_KEY'], # when you're running local\n",
    "#     )\n",
    "\n",
    "#     # embeddings\n",
    "#     embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#     embeddings_folder = \"./embedding_model/\"\n",
    "\n",
    "#     embeddings = HuggingFaceEmbedding(\n",
    "#         model_name=embedding_model,\n",
    "#         cache_folder=embeddings_folder,\n",
    "#     )\n",
    "\n",
    "#     # load Vector Database\n",
    "#     # allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
    "#     storage_context = StorageContext.from_defaults(persist_dir=\"./vector_index\")\n",
    "#     vector_index = load_index_from_storage(storage_context, embed_model=embeddings)\n",
    "\n",
    "#     # retriever\n",
    "#     retriever = vector_index.as_retriever(similarity_top_k=2)\n",
    "#     # memory\n",
    "#     memory = ChatMemoryBuffer.from_defaults()\n",
    "\n",
    "#     return ContextChatEngine(\n",
    "#         llm=llm, retriever=retriever, memory=memory, prefix_messages=prefix_messages\n",
    "#     )\n",
    "\n",
    "\n",
    "# rag_bot = init_bot()\n",
    "\n",
    "# ##### streamlit #####\n",
    "\n",
    "# st.title(\"Your Virtual Chef!\")\n",
    "\n",
    "\n",
    "# # Display chat messages from history on app rerun\n",
    "# for message in rag_bot.chat_history:\n",
    "#     with st.chat_message(message.role):\n",
    "#         st.markdown(message.blocks[0].text)\n",
    "\n",
    "# # React to user input\n",
    "# if prompt := st.chat_input(\"For Lovers of Gastronomie!\"): #:= creates a variable and store information in it at the same time.\n",
    "#     # Display user message in chat message container\n",
    "#     st.chat_message(\"human\").markdown(prompt)\n",
    "\n",
    "#     # Begin spinner before answering question so it's there for the duration\n",
    "#     with st.spinner(\"Going down the rabbithole for answers...\"):\n",
    "#         # send question to chain to get answer\n",
    "#         answer = rag_bot.chat(prompt)\n",
    "\n",
    "#         # extract answer from dictionary returned by chain\n",
    "#         response = answer.response\n",
    "\n",
    "#         # Display chatbot response in chat message container\n",
    "#         with st.chat_message(\"assistant\"):\n",
    "#             st.markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9a572",
   "metadata": {},
   "source": [
    "## Improved Streamlit Chatbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f057a0e",
   "metadata": {},
   "source": [
    "+ Optimization for being able to implement widgets in the chatbox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281119c",
   "metadata": {},
   "source": [
    "+ Prepare the code above to be able to run the Widgets in Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rag_app.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile rag_app.py  # WARNING: this code is working, just trying something new below.\n",
    "\n",
    "\n",
    "# from llama_index.llms.groq import Groq\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.core import StorageContext, load_index_from_storage\n",
    "# from llama_index.core.chat_engine import ContextChatEngine\n",
    "# from llama_index.core.memory import ChatMemoryBuffer\n",
    "# from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "# import streamlit as st\n",
    "\n",
    "# # Changing the chat background for an image:\n",
    "# from pathlib import Path\n",
    "# import base64\n",
    "\n",
    "# # Function to change the entire app background\n",
    "# def set_app_background(image_file):\n",
    "#     with open(image_file, \"rb\") as f:\n",
    "#         encoded = base64.b64encode(f.read()).decode()\n",
    "\n",
    "#     css = f\"\"\"\n",
    "#     <style>\n",
    "#     .stApp {{\n",
    "#         background-image: url(\"data:image/jpg;base64,{encoded}\");\n",
    "#         background-size: cover;\n",
    "#         background-position: center;\n",
    "#         background-repeat: no-repeat;\n",
    "#         background-attachment: fixed;\n",
    "#     }}\n",
    "#     </style>\n",
    "#     \"\"\"\n",
    "#     st.markdown(css, unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# def set_chat_background(image_file):\n",
    "#     with open(image_file, \"rb\") as f:\n",
    "#         encoded = base64.b64encode(f.read()).decode()\n",
    "# # CSS stands for Cascading Style Sheets, and it’s the language used to style and visually design web pages.\n",
    "\n",
    "#     css = f\"\"\"   \n",
    "#     <style>\n",
    "#     .stChatMessage {{\n",
    "#         background-image: url(\"data:image/jpg;base64,{encoded}\");\n",
    "#         background-size: cover;\n",
    "#         background-position: center;\n",
    "#         background-repeat: no-repeat;\n",
    "#         border-radius: 10px;\n",
    "#         padding: 10px;\n",
    "#     }}\n",
    "#     </style>\n",
    "#     \"\"\"\n",
    "#     st.markdown(css, unsafe_allow_html=True)\n",
    "\n",
    "# # Full app background\n",
    "# full_bg_path = Path(__file__).parent / \"kitchen_wall.jpg\"\n",
    "# set_app_background(str(full_bg_path))\n",
    "\n",
    "# # Call this function once at the top of your app. Assumes the file is in the same folder as the Jupyter notebook.\n",
    "# image_path = Path(__file__).parent / \"kitchen_background.jpg\"\n",
    "# set_chat_background(str(image_path))\n",
    "\n",
    "\n",
    "\n",
    "# # Setting up system prompt options:\n",
    "# prompt_options = {\n",
    "#     'basic_context': (\n",
    "#         'You are a chatbot with two modes: Beginner and Expert. '\n",
    "#         'You are a helpful chatbot having a conversation with a human. '\n",
    "#         ),\n",
    "#     'Beginner': (\n",
    "#         'YOU ARE NOW IN BEGINNER MODE, change your behavior if needed. '\n",
    "#         'You are a helpful chatbot having a conversation with a human. '\n",
    "#         'Look for easy to cook recipes. Ask always the number of people the user will cook for. Adjust your answer to that number of people. Present the ingredients first in bullet points, after that, the cooking instructions as detailed as possible and include preparation times'\n",
    "#         ),\n",
    "#     'Expert': (\n",
    "#         'YOU ARE NOW IN EXPERT MODE, change your behavior if needed. '\n",
    "#         'Search for complex recipes. Ask always the number of people the user will cook for. Adjust your answer to that number of people. Present the ingredients first in bullet points, after that, the cooking instructions with a low level of detail unless the user specify otherwise and include preparation times '\n",
    "#         )\n",
    "# }\n",
    "# # Setting up session state to store current system prompt setting\n",
    "# if 'system_prompts' not in st.session_state:\n",
    "#     st.session_state['system_prompts'] = ['basic_context'] #making it a list allow it to have multiple at once\n",
    "\n",
    "# ### INITIALIZING AND CACHING CHATBOT COMPONENTS ###\n",
    "\n",
    "# # Function for initializing the LLM\n",
    "# @st.cache_resource #the result will be cached so it only has to rerun when temp changes\n",
    "# def init_llm(temp=0.01):\n",
    "#     # LLM\n",
    "#     return Groq(\n",
    "#     model=\"llama-3.3-70b-versatile\",\n",
    "#     max_new_tokens=768,\n",
    "#     temperature=temp,\n",
    "#     top_p=0.95,\n",
    "#     repetition_penalty=1.03,\n",
    "#     token=st.secrets[\"GROQ_API_KEY\"]\n",
    "#     )\n",
    "\n",
    "# # Function for initializing the retriever\n",
    "# @st.cache_resource #the result will be cached so it only has to rerun when num_chunks changes\n",
    "# def init_rag(num_chunks=2):\n",
    "#     # RAG\n",
    "#     embeddings = HuggingFaceEmbedding(\n",
    "#         model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#         cache_folder=\"./embedding_model2/\",\n",
    "#     )\n",
    "#     storage_context = StorageContext.from_defaults(persist_dir=\"./vector_index\")\n",
    "#     vector_index = load_index_from_storage(storage_context, embed_model=embeddings)\n",
    "#     return vector_index.as_retriever(similarity_top_k=num_chunks)\n",
    "\n",
    "\n",
    "# # Function for initializing the chatbot memory\n",
    "# @st.cache_resource #the result will be cached so it only has to run once\n",
    "# def init_memory():\n",
    "#     return ChatMemoryBuffer.from_defaults()\n",
    "\n",
    "\n",
    "# # Function for initializing the bot with the specific settings\n",
    "# @st.cache_resource #the result is cached so, unless the parameters are altered, it doesn't need to recreate the bot\n",
    "# def init_bot(prefix_messages, temp=0.01, num_chunks=2):\n",
    "\n",
    "#     # This stuff is cached and only reruns if the parameters change\n",
    "#     llm = init_llm(temp) \n",
    "#     retriever = init_rag(num_chunks)\n",
    "#     memory = init_memory()\n",
    "\n",
    "#     # Takes the user selections in the session state and turns them into proper ChatMessages\n",
    "#     prefix_messages = [\n",
    "#         ChatMessage(\n",
    "#             role=MessageRole.SYSTEM,\n",
    "#             content=prompt_options[system_prompt_selection]\n",
    "#             if system_prompt_selection in prompt_options\n",
    "#             else system_prompt_selection\n",
    "#         )\n",
    "#         for system_prompt_selection in prefix_messages\n",
    "#     ]\n",
    "\n",
    "#     # Return initialized bot\n",
    "#     return ContextChatEngine(\n",
    "#         llm=llm, retriever=retriever, memory=memory, prefix_messages=prefix_messages\n",
    "#     )\n",
    "\n",
    "#     ##### STREAMLIT #####\n",
    "\n",
    "# st.title(\"Your Virtual Chef!\")\n",
    "\n",
    "\n",
    "# ### TEMPERATURE SLIDER ###\n",
    "# # temp = st.slider('Adjust the bot\\'s creativity level', 0.0, 2.0) # decide not to activate this.\n",
    "# temp = 0.1\n",
    "\n",
    "# ### PROMPT CUSTOMIZATION ###\n",
    "\n",
    "# # User can change the system prompts (see the dictionary above ^)\n",
    "# # if new_prompt := st.selectbox('Choose an attitude for the bot:', ['Helpful', 'Unhelpful']):\n",
    "# #     st.session_state['system_prompts'] = ['basic_context', new_prompt] #overwriting prompts instead of appending or swapping (for now)\n",
    "# # if new_prompt := st.radio('How confident are you with cooking?:', ['Beginner', 'Expert']):\n",
    "# #     st.session_state['system_prompts'] = ['basic_context', new_prompt] #overwriting prompts instead of appending or swapping (for now)\n",
    "\n",
    "\n",
    "# # Create two side by side columns\n",
    "# col1, col2 = st.columns(2)\n",
    "\n",
    "# # Cooking confidence (Beginner/Expert) -- Using radio\n",
    "\n",
    "# with col1:\n",
    "#     cooking_mode = st.radio('How confident are you with cooking?:', ['Beginner', 'Expert'], key= \"cooking_mode\")\n",
    "    \n",
    "\n",
    "\n",
    "# # Recipe type (Starter / Main Dish / Dessert) — using segmented control\n",
    "\n",
    "# with col2:\n",
    "#     recipe_type = st.segmented_control(\n",
    "#         \"Recipe type:\",\n",
    "#         [\"Starter\", \"Main Dish\", \"Dessert\"],\n",
    "#         key=\"recipe_type\",\n",
    "#         default=\"Main Dish\"  # 👈 ensures a value immediately\n",
    "#     )\n",
    "\n",
    "# st.session_state['system_prompts'] = [\n",
    "#     \"basic_context\",\n",
    "#     st.session_state.get(\"cooking_mode\", \"Beginner\"),\n",
    "#     f\"You are preparing a {recipe_type.lower()} recipe. Adjust your suggestions accordingly.\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# ### CHAT ###\n",
    "\n",
    "# # Initializing chatbot\n",
    "# # If the parameters change, this reruns, otherwise it uses what is in the cache already\n",
    "# rag_bot = init_bot(\n",
    "#     prefix_messages=st.session_state['system_prompts'],\n",
    "#     temp=temp,\n",
    "#     num_chunks=2\n",
    "# )\n",
    "\n",
    "# # Display chat messages from history on app rerun\n",
    "# for message in rag_bot.chat_history:\n",
    "#     with st.chat_message(message.role):\n",
    "#         st.markdown(message.blocks[0].text)\n",
    "\n",
    "\n",
    "# # React to user input\n",
    "# if prompt := st.chat_input('Reset the chat by typing \"Goodbye\"'):\n",
    "\n",
    "#     # If user types \"goodbye\", reset the memory and run the app from the top again\n",
    "#     if prompt.lower() == 'goodbye':\n",
    "#         rag_bot.reset() # reset the bot memory\n",
    "#         st.rerun() # reruns the app so that the bot is reinitialized and the chat is cleared\n",
    "    \n",
    "#     # Display user message in chat message container\n",
    "#     st.chat_message(\"human\").markdown(prompt)\n",
    "\n",
    "#     # Begin spinner before answering question so it's there for the duration\n",
    "#     with st.spinner(\"Be patient, a good meal requires always time!...\"):\n",
    "#         # send question to bot to get answer\n",
    "#         answer = rag_bot.chat(prompt)\n",
    "\n",
    "#         # extract answer from bot's response\n",
    "#         response = answer.response\n",
    "\n",
    "#         # Display chatbot response in chat message container\n",
    "#         with st.chat_message(\"assistant\"):\n",
    "#             st.markdown(response)\n",
    "# # Use streamlit run rag_app.py in Terminal to run this Python code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ba0262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rag_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rag_app.py\n",
    "\n",
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.chat_engine import ContextChatEngine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "import streamlit as st\n",
    "\n",
    "# Changing the chat background for an image:\n",
    "from pathlib import Path\n",
    "import base64\n",
    "\n",
    "# Function to change the entire app background\n",
    "def set_app_background(image_file):\n",
    "    with open(image_file, \"rb\") as f:\n",
    "        encoded = base64.b64encode(f.read()).decode()\n",
    "\n",
    "    css = f\"\"\"\n",
    "    <style>\n",
    "    .stApp {{\n",
    "        background-image: url(\"data:image/jpg;base64,{encoded}\");\n",
    "        background-size: cover;\n",
    "        background-position: center;\n",
    "        background-repeat: no-repeat;\n",
    "        background-attachment: fixed;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    st.markdown(css, unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "def set_chat_background(image_file):\n",
    "    with open(image_file, \"rb\") as f:\n",
    "        encoded = base64.b64encode(f.read()).decode()\n",
    "# CSS stands for Cascading Style Sheets, and it’s the language used to style and visually design web pages.\n",
    "\n",
    "    css = f\"\"\"   \n",
    "    <style>\n",
    "    .stChatMessage {{\n",
    "        background-image: url(\"data:image/jpg;base64,{encoded}\");\n",
    "        background-size: cover;\n",
    "        background-position: center;\n",
    "        background-repeat: no-repeat;\n",
    "        border-radius: 10px;\n",
    "        padding: 10px;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    st.markdown(css, unsafe_allow_html=True)\n",
    "\n",
    "# Full app background\n",
    "full_bg_path = Path(__file__).parent / \"kitchen_wall.jpg\"\n",
    "set_app_background(str(full_bg_path))\n",
    "\n",
    "# Call this function once at the top of your app. Assumes the file is in the same folder as the Jupyter notebook.\n",
    "image_path = Path(__file__).parent / \"kitchen_background.jpg\"\n",
    "set_chat_background(str(image_path))\n",
    "\n",
    "\n",
    "\n",
    "# Setting up system prompt options:\n",
    "prompt_options = {\n",
    "    'basic_context': (\n",
    "        'You are a chatbot with two modes: Beginner and Expert. '\n",
    "        'You are a helpful chatbot having a conversation with a human. '\n",
    "        ),\n",
    "    'Beginner': (\n",
    "        'YOU ARE NOW IN BEGINNER MODE, change your behavior if needed. '\n",
    "        'You are a helpful chatbot having a conversation with a human. '\n",
    "        'Look for easy to cook recipes. Ask always the number of people the user will cook for. Adjust your answer to that number of people. Present the ingredients first in bullet points, after that, the cooking instructions as detailed as possible and include preparation times'\n",
    "        ),\n",
    "    'Expert': (\n",
    "        'YOU ARE NOW IN EXPERT MODE, change your behavior if needed. '\n",
    "        'Search for complex recipes. Ask always the number of people the user will cook for. Adjust your answer to that number of people. Present the ingredients first in bullet points, after that, the cooking instructions with a low level of detail unless the user specify otherwise and include preparation times '\n",
    "        )\n",
    "}\n",
    "# Setting up session state to store current system prompt setting\n",
    "if 'system_prompts' not in st.session_state:\n",
    "    st.session_state['system_prompts'] = ['basic_context'] #making it a list allow it to have multiple at once\n",
    "\n",
    "### INITIALIZING AND CACHING CHATBOT COMPONENTS ###\n",
    "\n",
    "# Function for initializing the LLM\n",
    "@st.cache_resource #the result will be cached so it only has to rerun when temp changes\n",
    "def init_llm(temp=0.01):\n",
    "    # LLM\n",
    "    return Groq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    max_new_tokens=768,\n",
    "    temperature=temp,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.03,\n",
    "    token=st.secrets[\"GROQ_API_KEY\"]\n",
    "    )\n",
    "\n",
    "# Function for initializing the retriever\n",
    "@st.cache_resource #the result will be cached so it only has to rerun when num_chunks changes\n",
    "def init_rag(num_chunks=2):\n",
    "    # RAG\n",
    "    embeddings = HuggingFaceEmbedding(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        cache_folder=\"./embedding_model2/\",\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./vector_index\")\n",
    "    vector_index = load_index_from_storage(storage_context, embed_model=embeddings)\n",
    "    return vector_index.as_retriever(similarity_top_k=num_chunks)\n",
    "\n",
    "\n",
    "# Function for initializing the chatbot memory\n",
    "@st.cache_resource #the result will be cached so it only has to run once\n",
    "def init_memory():\n",
    "    return ChatMemoryBuffer.from_defaults()\n",
    "\n",
    "\n",
    "# Function for initializing the bot with the specific settings\n",
    "@st.cache_resource\n",
    "def init_bot(prefix_messages, temp=0.01, num_chunks=2):\n",
    "    # Initialize components\n",
    "    llm = init_llm(temp)\n",
    "    retriever = init_rag(num_chunks)\n",
    "    memory = init_memory()\n",
    "\n",
    "    # Build ChatMessage list safely\n",
    "    safe_prefix_messages = []\n",
    "    for system_prompt_selection in prefix_messages:\n",
    "        if system_prompt_selection in prompt_options:\n",
    "            # Use the predefined prompt text\n",
    "            content = prompt_options[system_prompt_selection]\n",
    "        else:\n",
    "            # Use the string as-is (dynamic prompt)\n",
    "            content = system_prompt_selection\n",
    "\n",
    "        safe_prefix_messages.append(\n",
    "            ChatMessage(\n",
    "                role=MessageRole.SYSTEM,\n",
    "                content=content\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Return initialized bot\n",
    "    return ContextChatEngine(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        prefix_messages=safe_prefix_messages\n",
    "    )\n",
    "\n",
    "\n",
    "    ##### STREAMLIT #####\n",
    "\n",
    "st.title(\"Your Virtual Chef!\")\n",
    "\n",
    "\n",
    "### PROMPT CUSTOMIZATION ###\n",
    "\n",
    "\n",
    "# Create two side by side columns\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "# Cooking confidence (Beginner/Expert) -- Using radio\n",
    "\n",
    "with col1:\n",
    "    st.radio('How confident are you with cooking? ', ['Beginner', 'Expert'], key= \"cooking_mode\")\n",
    "    \n",
    "\n",
    "# Recipe type (Starter / Main Dish / Dessert) — using segmented control\n",
    "\n",
    "with col2:\n",
    "    st.segmented_control(\n",
    "        \"Recipe type:\",\n",
    "        [\"Starter\", \"Main Dish\", \"Dessert\"],\n",
    "        key=\"recipe_type\",\n",
    "        default=\"Main Dish\"  # 👈 ensures a value immediately\n",
    "    )\n",
    "\n",
    "selected_mode = st.session_state.get(\"cooking_mode\", \"Beginner\")\n",
    "selected_type = st.session_state.get(\"recipe_type\", \"Main Dish\")\n",
    "\n",
    "st.session_state['system_prompts'] = [\n",
    "    \"basic_context\",\n",
    "    selected_mode,\n",
    "    f\"You are preparing a {selected_type.lower()} recipe. Adjust your suggestions accordingly.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "### CHAT ###\n",
    "\n",
    "# Initializing chatbot\n",
    "# If the parameters change, this reruns, otherwise it uses what is in the cache already\n",
    "rag_bot = init_bot(\n",
    "    prefix_messages=st.session_state['system_prompts'],\n",
    "    temp=0.5,\n",
    "    num_chunks=2\n",
    ")\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in rag_bot.chat_history:\n",
    "    with st.chat_message(message.role):\n",
    "        st.markdown(message.blocks[0].text)\n",
    "\n",
    "\n",
    "# React to user input\n",
    "if prompt := st.chat_input('Reset the chat by typing \"Goodbye\"'):\n",
    "\n",
    "    # If user types \"goodbye\", reset the memory and run the app from the top again\n",
    "    if prompt.lower() == 'goodbye':\n",
    "        rag_bot.reset() # reset the bot memory\n",
    "        st.rerun() # reruns the app so that the bot is reinitialized and the chat is cleared\n",
    "    \n",
    "    # Display user message in chat message container\n",
    "    st.chat_message(\"human\").markdown(prompt)\n",
    "\n",
    "    # Begin spinner before answering question so it's there for the duration\n",
    "    with st.spinner(\"Be patient, a good meal requires always time!...\"):\n",
    "        # send question to bot to get answer\n",
    "        answer = rag_bot.chat(prompt)\n",
    "\n",
    "        # extract answer from bot's response\n",
    "        response = answer.response\n",
    "\n",
    "        # Display chatbot response in chat message container\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(response)\n",
    "# Use streamlit run rag_app.py in Terminal to run this Python code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
